{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uplift_report.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d0rcsa/uplift-report/blob/internal-editable/uplift_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5mOAgeTNy0-o"
      },
      "source": [
        "# Remerge uplift report\n",
        "\n",
        "This notebook allows you to validate remerge provided uplift reporting numbers. To do so it downloads and analyses exported campaign and event data from S3. The campaign data contains all users that remerge marked to be part of an uplift test, the A/B group assignment, the timestamp of marking, conversion events (click, app open or similar) and their cost. The event data reflects the app event stream and includes events, their timestamp and revenue (if any). We calculate the incremental revenue and the iROAS in line with the [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). \n",
        "\n",
        "**Hint**: This notebook can be run in any Jupyter instance with enough space/memory, as a [Google Colab notebook](#Google-Colab-version) or as a standalone Python script. If you are using a copy of this notebook running on Colab or locally you can find the original template on [GitHub: remerge/uplift-report](https://github.com/remerge/uplift-report/blob/master/uplift_report_per_campaign.ipynb)\n",
        "\n",
        "### Notebook configuration\n",
        "\n",
        "For this notebook to work properly several variables in the [Configuration](#Configuration) section need to be be set: `customer`, `audience`, `\n",
        "revenue_event`, `dates` and the AWS credentials. All of these will be provided by your remerge account manager. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kyYz6TCny0-q"
      },
      "source": [
        "## Import packages\n",
        "\n",
        "This notebook/script needs our Uplift Report helper library, as long as several other dependencies it brings with it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9hjp-yR6MsK",
        "colab_type": "text"
      },
      "source": [
        "## Load helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoWjTAVwpvC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c68e9948-d2f3-4bf6-f1d2-5729198b4c3b"
      },
      "source": [
        "!pip install 'xxhash==1.3.0'\n",
        "!pip install 'pandas==0.24.2'\n",
        "!pip install 'scipy==1.3.0'\n",
        "!pip install 's3fs==0.3.0'\n",
        "!pip install 'pyarrow==0.14.0'\n",
        "!pip install 'partd==1.0.0'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xxhash==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/db/abd8ecd1753b60e5b527365676482bda272d71eaab0ad732a8be5f11d2d8/xxhash-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: xxhash\n",
            "Successfully installed xxhash-1.3.0\n",
            "Requirement already satisfied: pandas==0.24.2 in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas==0.24.2) (1.12.0)\n",
            "Collecting scipy==1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy==1.3.0) (1.16.4)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "  Found existing installation: scipy 1.3.1\n",
            "    Uninstalling scipy-1.3.1:\n",
            "      Successfully uninstalled scipy-1.3.1\n",
            "Successfully installed scipy-1.3.0\n",
            "Collecting s3fs==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/0e/9da790bcca3bff324da029ae2a2253f6be3b85787a811fa605a506d5dc92/s3fs-0.3.0.tar.gz (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3>=1.9.91 in /usr/local/lib/python3.6/dist-packages (from s3fs==0.3.0) (1.9.216)\n",
            "Requirement already satisfied: botocore>=1.12.91 in /usr/local/lib/python3.6/dist-packages (from s3fs==0.3.0) (1.12.216)\n",
            "Requirement already satisfied: fsspec>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from s3fs==0.3.0) (0.4.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3>=1.9.91->s3fs==0.3.0) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3>=1.9.91->s3fs==0.3.0) (0.2.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore>=1.12.91->s3fs==0.3.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore>=1.12.91->s3fs==0.3.0) (2.5.3)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore>=1.12.91->s3fs==0.3.0) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore>=1.12.91->s3fs==0.3.0) (1.12.0)\n",
            "Building wheels for collected packages: s3fs\n",
            "  Building wheel for s3fs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for s3fs: filename=s3fs-0.3.0-py2.py3-none-any.whl size=17751 sha256=b805371abd824ff09c247cdd3ff83bb4e89e10cc54330e74a056dd8365315564\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/5b/bb/5ab802653ead8635569805e742fffabccc01ec29517d445ae2\n",
            "Successfully built s3fs\n",
            "Installing collected packages: s3fs\n",
            "  Found existing installation: s3fs 0.3.3\n",
            "    Uninstalling s3fs-0.3.3:\n",
            "      Successfully uninstalled s3fs-0.3.3\n",
            "Successfully installed s3fs-0.3.0\n",
            "Collecting pyarrow==0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/af/9a4adfabb716f94e31da4ff5fa01eb37283d6b842bc1f92f50a5b662b02d/pyarrow-0.14.0-cp36-cp36m-manylinux1_x86_64.whl (31.6MB)\n",
            "\u001b[K     |████████████████████████████████| 31.6MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow==0.14.0) (1.16.4)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow==0.14.0) (1.12.0)\n",
            "Installing collected packages: pyarrow\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed pyarrow-0.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting partd==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8b/17/09c352519da1db31634979c3aa9126078e9ece0f561c5f641e0649b78905/partd-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from partd==1.0.0) (0.10.0)\n",
            "Collecting locket (from partd==1.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/22/3c0f97614e0be8386542facb3a7dcfc2584f7b83608c02333bced641281c/locket-0.2.0.tar.gz\n",
            "Building wheels for collected packages: locket\n",
            "  Building wheel for locket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for locket: filename=locket-0.2.0-cp36-none-any.whl size=4039 sha256=fa382a96fde3f14b68c11f7be763f2e80757829fa9a6ef35767e3e0a1d063db5\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/1e/e8/4fa236ec931b1a0cdd61578e20d4934d7bf188858723b84698\n",
            "Successfully built locket\n",
            "Installing collected packages: locket, partd\n",
            "Successfully installed locket-0.2.0 partd-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkZw79MIruGs",
        "colab_type": "text"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnGh2niupU4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "__version__ = '5.4.15-internal'\n",
        "\n",
        "# constants for groups\n",
        "TEST = True\n",
        "CONTROL = False\n",
        "\n",
        "# Helpers\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import xxhash\n",
        "import os\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import s3fs\n",
        "\n",
        "from IPython.display import display\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8u6Q76fCy0-u"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set the customer name, audience and access credentials for the S3 bucket and path. Furthermore the event for which we want to evaluate the uplift needs to be set `revenue_event`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wFgBi4jvsVTn",
        "colab": {}
      },
      "source": [
        "# configure path and revenue event \n",
        "customer = ''\n",
        "audiences = ['']\n",
        "revenue_event = 'purchase'\n",
        "\n",
        "# date range for the report\n",
        "dates = pd.date_range(start='2019-01-01',end='2019-01-01')\n",
        "\n",
        "# AWS credentials\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = ''\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = ''\n",
        "\n",
        "# Configure the reporting output: \n",
        "\n",
        "# named groups that aggregate several campaigns\n",
        "groups = {}\n",
        "\n",
        "# show uplift results per campaign:\n",
        "per_campaign_results = False\n",
        "\n",
        "# base statistical calculations on unique converters instead of conversions\n",
        "use_converters_for_significance = False\n",
        "\n",
        "# enable deduplication heuristic for appsflyer\n",
        "use_deduplication = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x45pJmcMpe9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cache_folder = \"cache-v{0}\".format(__version__)\n",
        "\n",
        "# columns to load from CSV\n",
        "bid_columns = ['ts', 'user_id', 'ab_test_group', 'campaign_id','cost_eur','event_type']\n",
        "attribution_columns = ['ts', 'user_id', 'partner_event', 'revenue_eur']  \n",
        "\n",
        "\n",
        "def load_marks_and_spend_data(customer, audiences, dates):\n",
        "    df = pd.concat([read_csv(customer, audience, 'marks_and_spend', date, columns=bid_columns) for audience in audiences for date in dates],\n",
        "                    ignore_index=True, verify_integrity=True)\n",
        "    return df\n",
        "\n",
        "def load_attribution_data(customer, audiences, dates, revenue_event, marks_and_spend_df, use_deduplication):\n",
        "    marked_user_ids = marked(marks_and_spend_df)['user_id']\n",
        "    df = pd.concat(\n",
        "    [filter_by_user_ids(read_csv(customer, audience, 'attributions', date, attribution_columns, revenue_event, extract_revenue_events), marked_user_ids) for audience in audiences for date in dates],\n",
        "    ignore_index=True, verify_integrity=True)\n",
        "\n",
        "    # AppsFlyer sends some events twice - we want to remove the duplicates before the analysis\n",
        "    if use_deduplication:\n",
        "        df = drop_duplicates_in_attributions(df, pd.Timedelta('1 minute'))\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def extract_revenue_events(df, revenue_event):\n",
        "    \"\"\"\n",
        "    Only keep rows where the event is a revenue event and drop the partner_event column afterwards\n",
        "    \"\"\"\n",
        "    df = df[df.partner_event == revenue_event]\n",
        "    return df.drop(columns=['partner_event'])\n",
        "\n",
        "\n",
        "def filter_by_user_ids(df, user_ids):\n",
        "    if 'user_id' in df.columns:\n",
        "        return df[df['user_id'].isin(user_ids)]\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "\n",
        "def improve_types(df):\n",
        "    \"\"\"\n",
        "    Use more memory efficient types for ts,user_id and ab_test_group\n",
        "    \"\"\"\n",
        "    df['ts'] = pd.to_datetime(df['ts'])\n",
        "    df['ts'] = (df['ts'].astype('int64') / 1e9).astype('int32')\n",
        "    df['user_id'] = df['user_id'].apply(xxhash.xxh64_intdigest).astype('int64')\n",
        "    if 'ab_test_group' in df.columns:\n",
        "        df['ab_test_group'] = df['ab_test_group'].transform(lambda g: g == 'test')\n",
        "    return df\n",
        "\n",
        "\n",
        "def path(customer, audience):\n",
        "    return \"s3://remerge-customers/{0}/uplift_data/{1}\".format(customer, audience)\n",
        "\n",
        "\n",
        "def to_parquet(df, filename):\n",
        "    \"\"\"\n",
        "    parquet save and load helper\n",
        "    \"\"\"\n",
        "    df.to_parquet(filename, engine='pyarrow')\n",
        "\n",
        "\n",
        "def from_parquet_corrected(filename, s3_filename, fs, columns):\n",
        "    \"\"\"\n",
        "    A little \"hack\" to convert old file on the fly\n",
        "    \"\"\"\n",
        "    df = from_parquet(filename)\n",
        "    update_cache = False\n",
        "    if columns:\n",
        "        to_drop = list(set(df.columns.values) - set(columns))\n",
        "        if to_drop:\n",
        "            df = df.drop(columns=to_drop)\n",
        "            update_cache = True\n",
        "\n",
        "    # remove events without a user id\n",
        "    if df['user_id'].dtype == 'object':\n",
        "        if df[df['user_id'].isnull()].empty == False or df[df['user_id'].str.len() != 36].empty == False:\n",
        "            df = df[df['user_id'].str.len() == 36]\n",
        "            update_cache = True\n",
        "\n",
        "    if df['user_id'].dtype != 'int64':\n",
        "        df = improve_types(df)\n",
        "        update_cache = True\n",
        "\n",
        "    if update_cache:\n",
        "        print(datetime.now(), 'rewritting cached file with correct types (local and S3)', filename, s3_filename)\n",
        "        to_parquet(df, filename)\n",
        "        fs.put(filename, s3_filename)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def from_parquet(filename):\n",
        "    return pd.read_parquet(filename, engine='pyarrow')\n",
        "\n",
        "\n",
        "def read_csv(customer, audience, source, date, columns=None, revenue_event=None, chunk_filter_fn=None,\n",
        "             chunk_size=10 ** 6):\n",
        "    \"\"\"\n",
        "    Helper to download CSV files, convert to DF and print time needed.\n",
        "    Caches files locally and on S3 to be reused.\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "\n",
        "    date_str = date.strftime('%Y%m%d')\n",
        "\n",
        "    filename = '{0}/{1}/{2}.csv.gz'.format(path(customer, audience), source, date_str)\n",
        "\n",
        "    # local cache\n",
        "    cache_dir = '{0}/{1}/{2}'.format(cache_folder, audience, source)\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    cache_filename = '{0}/{1}.parquet'.format(cache_dir, date_str)\n",
        "\n",
        "    # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "    s3_cache_filename = '{0}/{1}/{2}/{3}.parquet'.format(path(customer, audience), source, cache_folder, date_str)\n",
        "\n",
        "    if source == 'attributions':\n",
        "        cache_filename = '{0}/{1}-{2}.parquet'.format(cache_dir, date_str, revenue_event)\n",
        "\n",
        "        # s3 cache (useful if we don't have enough space on the Colab instance)\n",
        "        s3_cache_filename = '{0}/{1}/{2}/{3}-{4}.parquet' \\\n",
        "            .format(path(customer, audience), source, cache_folder, date_str, revenue_event)\n",
        "\n",
        "    fs = s3fs.S3FileSystem(anon=False)\n",
        "    fs.connect_timeout = 10  # defaults to 5\n",
        "    fs.read_timeout = 30  # defaults to 15 \n",
        "\n",
        "    if os.path.exists(cache_filename):\n",
        "        print(now, 'loading from', cache_filename)\n",
        "        return from_parquet_corrected(cache_filename, s3_cache_filename, fs, columns)\n",
        "\n",
        "    if fs.exists(path=s3_cache_filename):\n",
        "        print(now, 'loading from S3 cache', s3_cache_filename)\n",
        "\n",
        "        # Download the file to local cache first to avoid timeouts during the load.\n",
        "        # This way, if they happen, restart will be using local copies first.\n",
        "        fs.get(s3_cache_filename, cache_filename)\n",
        "\n",
        "        print(now, 'stored S3 cache file to local drive, loading', cache_filename)\n",
        "\n",
        "        return from_parquet_corrected(cache_filename, s3_cache_filename, fs, columns)\n",
        "\n",
        "    print(now, 'start loading CSV for', audience, source, date)\n",
        "\n",
        "    read_csv_kwargs = {'chunksize': chunk_size}\n",
        "    if columns:\n",
        "        read_csv_kwargs['usecols'] = columns\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    if not fs.exists(path=filename):\n",
        "        print(now, 'WARNING: no CSV file at for: ', audience, source, date, ', skipping the file: ', filename)\n",
        "        return df\n",
        "\n",
        "    for chunk in pd.read_csv(filename, escapechar='\\\\', low_memory=False, **read_csv_kwargs):\n",
        "        if chunk_filter_fn:\n",
        "            filtered_chunk = chunk_filter_fn(chunk, revenue_event)\n",
        "        else:\n",
        "            filtered_chunk = chunk\n",
        "\n",
        "        # remove events without a user id\n",
        "        filtered_chunk = filtered_chunk[filtered_chunk['user_id'].str.len() == 36]\n",
        "\n",
        "        filtered_chunk = improve_types(filtered_chunk)\n",
        "\n",
        "        df = pd.concat([df, filtered_chunk],\n",
        "                       ignore_index=True, verify_integrity=True)\n",
        "\n",
        "    print(datetime.now(), 'finished loading CSV for', date.strftime('%d.%m.%Y'),\n",
        "          'took', datetime.now() - now)\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    print(datetime.now(), 'caching local as parquet', cache_filename)\n",
        "    to_parquet(df, cache_filename)\n",
        "\n",
        "    # write it to the S3 cache folder as well\n",
        "    print(datetime.now(), 'caching on S3 as parquet', s3_cache_filename)\n",
        "    to_parquet(df, s3_cache_filename)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def calculate_ad_spend(df):\n",
        "    ad_spend_micros = df[(df.event_type == 'buying_conversion') & (df.ab_test_group == TEST)]['cost_eur'].sum()\n",
        "    return ad_spend_micros / 10 ** 6\n",
        "\n",
        "\n",
        "def marked(df):\n",
        "    \"\"\"\n",
        "    The dataframe created by `marked` will contain all mark events. Remerge marks users per campaign. If a user was\n",
        "    marked once for an audience he will have the same group allocation for consecutive marks (different campaigns)\n",
        "    unless manually reset on audience level.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "    \n",
        "    mark_df = df[df.event_type == 'mark']\n",
        "\n",
        "    # we dont need the event_type anymore (to save memory)\n",
        "    mark_df = mark_df.drop(columns=['event_type'])\n",
        "\n",
        "    sorted_mark_df = mark_df.sort_values('ts')\n",
        "\n",
        "    depuplicated_mark_df = sorted_mark_df.drop_duplicates(['user_id'])\n",
        "\n",
        "    return depuplicated_mark_df\n",
        "\n",
        "\n",
        "def merge(mark_df, attributions_df):\n",
        "    \"\"\"\n",
        "    `merge` joins the marked users with the revenue events and excludes any revenue event that happened before the\n",
        "    user was marked.\n",
        "    \"\"\"\n",
        "    merged_df = pd.merge(attributions_df, mark_df, on='user_id')\n",
        "\n",
        "    return merged_df[merged_df.ts_x > merged_df.ts_y]\n",
        "\n",
        "\n",
        "def drop_duplicates_in_attributions(df, max_timedelta):\n",
        "    \"\"\"\n",
        "    # Clean the data\n",
        "    Due to some inconsistencies in the measurement we need to clean the data before analysis.\n",
        "    ### Remove duplicated events coming from AppsFlyer\n",
        "    AppsFlyer is sending us two revenue events if they attribute the event to us. One of the events they send us does\n",
        "    not contain attribution information and the other one does. Sadly, it is not possible for us to distinguish\n",
        "    correctly if an event is a duplicate or if the user actually triggered two events with nearly the same information.\n",
        "    Therefore we rely on a heuristic. We consider an event a duplicate if the user and revenue are equal and the events\n",
        "    are less than a minute apart.\n",
        "    \"\"\"\n",
        "    sorted = df.sort_values(['user_id', 'revenue_eur'])\n",
        "\n",
        "    # Get values of the previous row\n",
        "    sorted['last_ts'] = sorted['ts'].shift(1)\n",
        "    sorted['last_user_id'] = sorted['user_id'].shift(1)\n",
        "    sorted['last_revenue'] = sorted['revenue_eur'].shift(1)\n",
        "\n",
        "    # Remove rows if the previous row has the same revenue_eur and user id and the ts are less than max_timedelta apart\n",
        "    filtered = sorted[\n",
        "        (sorted['user_id'] != sorted['last_user_id']) |\n",
        "        (sorted['revenue_eur'] != sorted['last_revenue']) |\n",
        "        ((pd.to_datetime(sorted['ts']) - pd.to_datetime(sorted['last_ts'])) > max_timedelta)]\n",
        "\n",
        "    return filtered[['ts', 'user_id', 'revenue_eur']]\n",
        "\n",
        "\n",
        "def uplift(marks_and_spend_df, attributions_df, index_name, use_converters_for_significance, m_hypothesis=1):\n",
        "    \"\"\"\n",
        "    # Uplift Calculation\n",
        "    We calculate the incremental revenue and the iROAS in line with the\n",
        "    [remerge whitepaper](https://drive.google.com/file/d/1PTJ93Cpjw1BeiVns8dTcs2zDDWmmjpdc/view). Afterwards we run a\n",
        "    [chi squared test](https://en.wikipedia.org/wiki/Chi-squared_test) on the results to test for significance of the\n",
        "    results, comparing conversion to per group uniques.\n",
        "    \"\"\"\n",
        "    # filter for mark events\n",
        "    marks_df = marked(marks_and_spend_df)\n",
        "\n",
        "    # calculate group sizes\n",
        "    test_group_size = marks_df[marks_df['ab_test_group'] == TEST]['user_id'].nunique()\n",
        "    if test_group_size == 0:\n",
        "        print(\"WARNING: No users marked as test for \", index_name, 'skipping.. ')\n",
        "        return None\n",
        "\n",
        "    control_group_size = marks_df[marks_df['ab_test_group'] == CONTROL]['user_id'].nunique()\n",
        "    if control_group_size == 0:\n",
        "        print(\"WARNING: No users marked as control for \", index_name, 'skipping.. ')\n",
        "        return None\n",
        "\n",
        "    # join marks and revenue events    \n",
        "    merged_df = merge(marks_df, attributions_df)\n",
        "    grouped_revenue = merged_df.groupby(by='ab_test_group')\n",
        "\n",
        "    # init all KPIs with 0s first:\n",
        "    test_revenue_micros = 0\n",
        "    test_conversions = 0\n",
        "    test_converters = 0\n",
        "\n",
        "    control_revenue_micros = 0\n",
        "    control_conversions = 0\n",
        "    control_converters = 0\n",
        "\n",
        "    # we might not have any events for a certain group in the time-period,\n",
        "    if TEST in grouped_revenue.groups:\n",
        "        test_revenue_df = grouped_revenue.get_group(TEST)\n",
        "        test_revenue_micros = test_revenue_df['revenue_eur'].sum()\n",
        "        # test_conversions = test_revenue_df['partner_event'].count()\n",
        "        # as we filtered by revenue event and dropped the column we can just use\n",
        "        test_conversions = test_revenue_df['user_id'].count()\n",
        "        test_converters = test_revenue_df['user_id'].nunique()\n",
        "\n",
        "    if CONTROL in grouped_revenue.groups:\n",
        "        control_revenue_df = grouped_revenue.get_group(CONTROL)\n",
        "        control_revenue_micros = control_revenue_df['revenue_eur'].sum()\n",
        "        # control_conversions = control_revenue_df['partner_event'].count()\n",
        "        # as we filtered by revenue event and dropped the column we can just use\n",
        "        control_conversions = control_revenue_df['user_id'].count()\n",
        "        control_converters = control_revenue_df['user_id'].nunique()\n",
        "\n",
        "    # calculate KPIs\n",
        "    test_revenue = test_revenue_micros / 10 ** 6\n",
        "    control_revenue = control_revenue_micros / 10 ** 6\n",
        "\n",
        "    ratio = float(test_group_size) / float(control_group_size)\n",
        "    scaled_control_conversions = float(control_conversions) * ratio\n",
        "    scaled_control_revenue_micros = float(control_revenue_micros) * ratio\n",
        "    incremental_conversions = test_conversions - scaled_control_conversions\n",
        "    incremental_revenue_micros = test_revenue_micros - scaled_control_revenue_micros\n",
        "    incremental_revenue = incremental_revenue_micros / 10 ** 6\n",
        "    incremental_converters = test_converters - control_converters * ratio\n",
        "\n",
        "    # calculate the ad spend        \n",
        "    ad_spend = calculate_ad_spend(marks_and_spend_df)\n",
        "\n",
        "    iroas = incremental_revenue / ad_spend\n",
        "    icpa = ad_spend / incremental_conversions\n",
        "    cost_per_incremental_converter = ad_spend / incremental_converters\n",
        "\n",
        "    rev_per_conversion_test = 0\n",
        "    rev_per_conversion_control = 0\n",
        "    if test_conversions > 0:\n",
        "        rev_per_conversion_test = test_revenue / test_conversions\n",
        "    if control_conversions > 0:\n",
        "        rev_per_conversion_control = control_revenue / control_conversions\n",
        "\n",
        "    test_cvr = test_conversions / test_group_size\n",
        "    control_cvr = control_conversions / control_group_size\n",
        "\n",
        "    uplift = 0\n",
        "    if control_cvr > 0:\n",
        "        uplift = test_cvr / control_cvr - 1\n",
        "\n",
        "    # calculate statistical significance\n",
        "    control_successes, test_successes = control_conversions, test_conversions\n",
        "    if use_converters_for_significance or max(test_cvr, control_cvr) > 1.0:\n",
        "        control_successes, test_successes = control_converters, test_converters\n",
        "    chi_df = pd.DataFrame({\n",
        "        \"conversions\": [control_successes, test_successes],\n",
        "        \"total\": [control_group_size, test_group_size]\n",
        "    }, index=['control', 'test'])\n",
        "    # CHI square calculation will fail with insufficient data\n",
        "    # Fallback to no significance\n",
        "    try:\n",
        "        chi, p, _, _ = scipy.stats.chi2_contingency(\n",
        "            pd.concat([chi_df.total - chi_df.conversions, chi_df.conversions], axis=1), correction=False)\n",
        "    except:\n",
        "        chi, p = 0, 1.0\n",
        "\n",
        "    # bonferroni correction with equal weights - if we have multiple hypothesis:\n",
        "    # https://en.wikipedia.org/wiki/Bonferroni_correction\n",
        "    significant = p < 0.05 / m_hypothesis\n",
        "\n",
        "    dataframe_dict = {\n",
        "        \"ad spend\": ad_spend,\n",
        "        \"total revenue\": test_revenue + control_revenue,\n",
        "        \"test group size\": test_group_size,\n",
        "        \"test conversions\": test_conversions,\n",
        "        \"test converters\": test_converters,\n",
        "        \"test revenue\": test_revenue,\n",
        "        \"control group size\": control_group_size,\n",
        "        \"control conversions\": control_conversions,\n",
        "        \"control_converters\": control_converters,\n",
        "        \"control revenue\": control_revenue,\n",
        "        \"ratio test/control\": ratio,\n",
        "        \"control conversions (scaled)\": scaled_control_conversions,\n",
        "        \"control revenue (scaled)\": scaled_control_revenue_micros / 10 ** 6,\n",
        "        \"incremental conversions\": incremental_conversions,\n",
        "        \"incremental converters\": incremental_converters,\n",
        "        \"incremental revenue\": incremental_revenue,\n",
        "        \"rev/conversions test\": rev_per_conversion_test,\n",
        "        \"rev/conversions control\": rev_per_conversion_control,\n",
        "        \"test CVR\": test_cvr,\n",
        "        \"control CVR\": control_cvr,\n",
        "        \"CVR Uplift\": uplift,\n",
        "        \"iROAS\": iroas,\n",
        "        \"cost per incr. converter\": cost_per_incremental_converter,\n",
        "        \"iCPA\": icpa,\n",
        "        \"chi^2\": chi,\n",
        "        \"p-value\": p,\n",
        "        \"significant\": significant\n",
        "    }\n",
        "\n",
        "    # show results as a dataframe\n",
        "    return pd.DataFrame(\n",
        "        dataframe_dict,\n",
        "        index=[index_name],\n",
        "    ).transpose()\n",
        "\n",
        "\n",
        "def uplift_report(marks_and_spend_df, attributions_df, groups, per_campaign_results, use_converters_for_significance):\n",
        "    \"\"\"\n",
        "    Calculate and display uplift report for the data set as a whole\n",
        "    This takes the whole data set and calculates uplift KPIs.\n",
        "    \"\"\"\n",
        "    # calculate the total result:\n",
        "    report_df = uplift(marks_and_spend_df, attributions_df, \"total\", use_converters_for_significance)\n",
        "\n",
        "    # if there are groups filter the events against the per campaign groups and generate report\n",
        "    if report_df is not None and len(groups) > 0:\n",
        "        for name, campaigns in groups.items():\n",
        "            group_df = marks_and_spend_df[marks_and_spend_df.campaign_id.isin(campaigns)]\n",
        "            report_df[name] = uplift(group_df, attributions_df, name, use_converters_for_significance, len(groups))\n",
        "\n",
        "    if report_df is not None and per_campaign_results:\n",
        "        campaigns = marks_and_spend_df['campaign_id'].unique()\n",
        "        for campaign in campaigns:\n",
        "            name = \"c_{0}\".format(campaign)\n",
        "            campaign_df = marks_and_spend_df[marks_and_spend_df.campaign_id == campaign]\n",
        "            report_df[name] = uplift(campaign_df, attributions_df, name, use_converters_for_significance,\n",
        "                                     len(campaigns))\n",
        "    return report_df\n",
        "\n",
        "\n",
        "def export_csv(df, file_name):\n",
        "    df.to_csv(file_name)\n",
        "\n",
        "    print('Stored results as a local CSV file: %s' % file_name)\n",
        "\n",
        "    try:\n",
        "        import google.colab\n",
        "\n",
        "        print('The download of the results file should start automatically')\n",
        "        google.colab.files.download(file_name)\n",
        "    except ImportError:\n",
        "        # We are not in the collab, no need to run the download\n",
        "        pass\n",
        "\n",
        "def overview_row(customer, audiences, dates, total):\n",
        "    return list([\n",
        "        customer,\n",
        "        \",\".join(audiences),\n",
        "        dates[0].strftime('%Y-%m-%d'),\n",
        "        dates[-1].strftime('%Y-%m-%d'),\n",
        "        __version__,\n",
        "    ]) + list(total.values)\n",
        "\n",
        "def export_to_overview(customer, audiences, dates, report):\n",
        "    auth.authenticate_user()\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "    worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1je3b1g6Yg2B-bmwx6CP5akcrsj9DzQcs9PnKHoKoioQ/edit').sheet1\n",
        "    row = overview_row(customer, audiences, dates, report['total'])\n",
        "    worksheet.append_row(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZI-WE9T3edT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export to google sheets\n",
        "\n",
        "def export_to_overview(customer, audiences, dates, report):\n",
        "    auth.authenticate_user()\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "    worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1je3b1g6Yg2B-bmwx6CP5akcrsj9DzQcs9PnKHoKoioQ/edit').sheet1\n",
        "    row = overview_row(customer, audiences, dates, report['total'])\n",
        "    worksheet.append_row(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Novl387nrno"
      },
      "source": [
        "## Version\n",
        "Version of the analysis script corresponding to the methodology version in the whitepaper (Major + Minor version represent the whitepaper version, revision represents changes and fixes of the uplift report script)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U2vm5Z9UoHe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f0c35ef0-97e6-4c68-a229-b101b663f696"
      },
      "source": [
        "display(__version__)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'5.4.15-internal'"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eSixTLyiy0_A"
      },
      "source": [
        "## Load CSV data from S3\n",
        "\n",
        "Load mark, spend and event data from S3. \n",
        "\n",
        "### IMPORTANT\n",
        "\n",
        "**The event data is usually quite large (several GB) so this operation might take several minutes or hours to complete, depending on the size and connection.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcm2nq4G5SUs",
        "colab_type": "text"
      },
      "source": [
        "### Deduplication for appsflyer\n",
        "AppsFlyer sends some events twice - we want to remove the duplicates before the analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQpShRFc5SUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    # AppsFlyer sends some events twice - we want to remove the duplicates before the analysis\n",
        "     if use_deduplication:\n",
        "        df = drop_duplicates_in_attributions(df, pd.Timedelta('1 minute'))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tKdBRvkxL8Aa"
      },
      "source": [
        "### Calculate and display uplift report for the data set as a whole\n",
        "\n",
        "This takes the whole data set and calculates uplift KPIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-74tjdPMDvg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "4659f689-9448-4a4b-d676-40ce43ef607d"
      },
      "source": [
        "marks_and_spend_df = load_marks_and_spend_data(customer, audiences, dates)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-03 12:31:07.433842 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/marks_and_spend/cache-v5.4.15-internal/20190820.parquet\n",
            "2019-09-03 12:31:07.433842 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/marks_and_spend/20190820.parquet\n",
            "2019-09-03 12:31:08.107843 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/marks_and_spend/cache-v5.4.15-internal/20190821.parquet\n",
            "2019-09-03 12:31:08.107843 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/marks_and_spend/20190821.parquet\n",
            "2019-09-03 12:31:08.334577 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/marks_and_spend/cache-v5.4.15-internal/20190822.parquet\n",
            "2019-09-03 12:31:08.334577 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/marks_and_spend/20190822.parquet\n",
            "2019-09-03 12:31:08.569663 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/marks_and_spend/cache-v5.4.15-internal/20190823.parquet\n",
            "2019-09-03 12:31:08.569663 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/marks_and_spend/20190823.parquet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs8qYb1fD1pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "5066cf32-c5eb-441b-8158-3577646ce03b"
      },
      "source": [
        "attributions_df = load_attribution_data(customer, audiences, dates, revenue_event, marks_and_spend_df, use_deduplication)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-03 12:31:10.448987 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/attributions/cache-v5.4.15-internal/20190820-Purchase.parquet\n",
            "2019-09-03 12:31:10.448987 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/attributions/20190820-Purchase.parquet\n",
            "2019-09-03 12:31:10.777529 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/attributions/cache-v5.4.15-internal/20190821-Purchase.parquet\n",
            "2019-09-03 12:31:10.777529 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/attributions/20190821-Purchase.parquet\n",
            "2019-09-03 12:31:10.991092 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/attributions/cache-v5.4.15-internal/20190822-Purchase.parquet\n",
            "2019-09-03 12:31:10.991092 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/attributions/20190822-Purchase.parquet\n",
            "2019-09-03 12:31:11.241755 loading from S3 cache s3://remerge-customers/lovoo-2/uplift_data/3_Lovoo_iOS/attributions/cache-v5.4.15-internal/20190823-Purchase.parquet\n",
            "2019-09-03 12:31:11.241755 stored S3 cache file to local drive, loading cache-v5.4.15-internal/3_Lovoo_iOS/attributions/20190823-Purchase.parquet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0Ih6SSuy0_J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "531c30a2-41cf-4349-9a45-9d2da64c67c3"
      },
      "source": [
        "marks_and_spend_df.info(memory_usage='deep')\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 71232 entries, 0 to 71231\n",
            "Data columns (total 6 columns):\n",
            "ts               71232 non-null int32\n",
            "event_type       71232 non-null object\n",
            "ab_test_group    71232 non-null bool\n",
            "user_id          71232 non-null int64\n",
            "campaign_id      71232 non-null int64\n",
            "cost_eur         631 non-null float64\n",
            "dtypes: bool(1), float64(1), int32(1), int64(2), object(1)\n",
            "memory usage: 6.1 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ec_qFUaVy0_I"
      },
      "source": [
        "Print some statistics of the loaded data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EoU_cW07y0_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "eea4a063-e287-4ddf-caa8-c8672b61f6e3"
      },
      "source": [
        "attributions_df.info(memory_usage='deep')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8680 entries, 0 to 8679\n",
            "Data columns (total 3 columns):\n",
            "ts             8680 non-null int32\n",
            "user_id        8680 non-null int64\n",
            "revenue_eur    8578 non-null float64\n",
            "dtypes: float64(1), int32(1), int64(1)\n",
            "memory usage: 169.6 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SdFSmL3u8Pe4"
      },
      "source": [
        "## Uplift Results\n",
        "\n",
        "You can configure the ouput by using variables in the 'Configuration' section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SjX4uu6tTpK2",
        "colab": {}
      },
      "source": [
        "report = uplift_report(marks_and_spend_df, attributions_df, groups, per_campaign_results, use_converters_for_significance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWQXKYXB8YO2",
        "colab": {}
      },
      "source": [
        "# set formatting options\n",
        "pd.set_option('display.float_format', '{:.3f}'.format)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S2UZOvmlaXqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "080dd08b-2856-4355-b1e2-5e14148f215d"
      },
      "source": [
        "display(report)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total</th>\n",
              "      <th>c_18789</th>\n",
              "      <th>c_18784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ad spend</th>\n",
              "      <td>272.540</td>\n",
              "      <td>100.940</td>\n",
              "      <td>171.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total revenue</th>\n",
              "      <td>83138.012</td>\n",
              "      <td>73784.635</td>\n",
              "      <td>9393.838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test group size</th>\n",
              "      <td>25487</td>\n",
              "      <td>19471</td>\n",
              "      <td>6069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test conversions</th>\n",
              "      <td>5175</td>\n",
              "      <td>4640</td>\n",
              "      <td>538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test converters</th>\n",
              "      <td>1005</td>\n",
              "      <td>818</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test revenue</th>\n",
              "      <td>76110.205</td>\n",
              "      <td>67141.119</td>\n",
              "      <td>9009.547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control group size</th>\n",
              "      <td>2820</td>\n",
              "      <td>2109</td>\n",
              "      <td>718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control conversions</th>\n",
              "      <td>331</td>\n",
              "      <td>299</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control_converters</th>\n",
              "      <td>108</td>\n",
              "      <td>87</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control revenue</th>\n",
              "      <td>7027.807</td>\n",
              "      <td>6643.516</td>\n",
              "      <td>384.291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ratio test/control</th>\n",
              "      <td>9.038</td>\n",
              "      <td>9.232</td>\n",
              "      <td>8.453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control conversions (scaled)</th>\n",
              "      <td>2991.559</td>\n",
              "      <td>2760.469</td>\n",
              "      <td>270.485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control revenue (scaled)</th>\n",
              "      <td>63516.922</td>\n",
              "      <td>61335.183</td>\n",
              "      <td>3248.276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incremental conversions</th>\n",
              "      <td>2183.441</td>\n",
              "      <td>1879.531</td>\n",
              "      <td>267.515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incremental converters</th>\n",
              "      <td>28.902</td>\n",
              "      <td>14.787</td>\n",
              "      <td>11.494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incremental revenue</th>\n",
              "      <td>12593.283</td>\n",
              "      <td>5805.935</td>\n",
              "      <td>5761.271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rev/conversions test</th>\n",
              "      <td>14.707</td>\n",
              "      <td>14.470</td>\n",
              "      <td>16.746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rev/conversions control</th>\n",
              "      <td>21.232</td>\n",
              "      <td>22.219</td>\n",
              "      <td>12.009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test CVR</th>\n",
              "      <td>0.203</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>control CVR</th>\n",
              "      <td>0.117</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CVR Uplift</th>\n",
              "      <td>0.730</td>\n",
              "      <td>0.681</td>\n",
              "      <td>0.989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>iROAS</th>\n",
              "      <td>46.207</td>\n",
              "      <td>57.519</td>\n",
              "      <td>33.574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cost per incr. converter</th>\n",
              "      <td>9.430</td>\n",
              "      <td>6.826</td>\n",
              "      <td>14.929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>iCPA</th>\n",
              "      <td>0.125</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chi^2</th>\n",
              "      <td>118.937</td>\n",
              "      <td>100.466</td>\n",
              "      <td>16.215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p-value</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>significant</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 total   c_18789  c_18784\n",
              "ad spend                       272.540   100.940  171.600\n",
              "total revenue                83138.012 73784.635 9393.838\n",
              "test group size                  25487     19471     6069\n",
              "test conversions                  5175      4640      538\n",
              "test converters                   1005       818      189\n",
              "test revenue                 76110.205 67141.119 9009.547\n",
              "control group size                2820      2109      718\n",
              "control conversions                331       299       32\n",
              "control_converters                 108        87       21\n",
              "control revenue               7027.807  6643.516  384.291\n",
              "ratio test/control               9.038     9.232    8.453\n",
              "control conversions (scaled)  2991.559  2760.469  270.485\n",
              "control revenue (scaled)     63516.922 61335.183 3248.276\n",
              "incremental conversions       2183.441  1879.531  267.515\n",
              "incremental converters          28.902    14.787   11.494\n",
              "incremental revenue          12593.283  5805.935 5761.271\n",
              "rev/conversions test            14.707    14.470   16.746\n",
              "rev/conversions control         21.232    22.219   12.009\n",
              "test CVR                         0.203     0.238    0.089\n",
              "control CVR                      0.117     0.142    0.045\n",
              "CVR Uplift                       0.730     0.681    0.989\n",
              "iROAS                           46.207    57.519   33.574\n",
              "cost per incr. converter         9.430     6.826   14.929\n",
              "iCPA                             0.125     0.054    0.641\n",
              "chi^2                          118.937   100.466   16.215\n",
              "p-value                          0.000     0.000    0.000\n",
              "significant                       True      True     True"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6VA_k2BobaZS"
      },
      "source": [
        "### CSV Export - combined reports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K-T6quwwbObO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "993f1138-d379-49e0-e941-0f64d35b424f"
      },
      "source": [
        "start = dates[0]\n",
        "end = dates[-1]\n",
        "export_csv(report,'{}_{}-{}.csv'.format(customer, start, end))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stored results as a local CSV file: lovoo-2_2019-08-20 00:00:00-2019-08-23 00:00:00.csv\n",
            "The download of the results file should start automatically\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N3Os_rq8_xL",
        "colab_type": "text"
      },
      "source": [
        "### Group by Date and export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA-PsIKd791y",
        "colab_type": "code",
        "outputId": "198e4210-5fb9-4dc3-9c89-980e33eaf354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#calculate conversions, customer revnenue grouped\n",
        "\n",
        "\n",
        "def group_conv_date(marks_and_spend_df, attributions_df): \n",
        "\n",
        "    merged_df = merge(marked(marks_and_spend_df),attributions_df)\n",
        "    merged_df['ts_x']  = pd.to_datetime(merged_df['ts_x'],unit='s').dt.date\n",
        "    merged_df['conversions'] = merged_df['user_id']\n",
        "    grouped = merged_df.groupby(['ts_x','campaign_id','ab_test_group'])\n",
        "    grouped_conv_df = grouped.agg({'revenue_eur':'sum','user_id' :'nunique', 'conversions': 'count'}).reset_index()\n",
        "    grouped_conv_df['customer revenue'] = grouped_conv_df['revenue_eur'] / 10 ** 6\n",
        "    return pd.DataFrame(grouped_conv_df).drop(['revenue_eur'], axis =1).rename(columns = { 'ts_x': 'ts', 'ab_test_group':'ab_test_group', 'user_id':'converters'})\n",
        "                       \n",
        "table2_df = group_conv_date(marks_and_spend_df,attributions_df)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "#calculating both group size and ad spend from marks_and_spend_df\n",
        "\n",
        "\n",
        "def group_spend_size (marks_and_spend_df) :\n",
        "  marks_and_spend_df['ts']  = pd.to_datetime(marks_and_spend_df['ts'],unit='s').dt.date\n",
        "  grouped = marks_and_spend_df.groupby(['ts','campaign_id','ab_test_group'])\n",
        "  grouped_spend_size_df = grouped.agg({'cost_eur':'sum','user_id':'nunique'}).reset_index()\n",
        "  grouped_spend_size_df['ad spend']=grouped_spend_size_df['cost_eur'] / 10 ** 6\n",
        "  return pd.DataFrame(grouped_spend_size_df).drop(['cost_eur'], axis =1).rename(columns={'user_id': 'group size'})\n",
        "\n",
        "table1_df = group_spend_size(marks_and_spend_df)\n",
        "\n",
        "table3_df = pd.merge(table2_df, table1_df, on = ['ts','campaign_id','ab_test_group'])\n",
        "print(pd.DataFrame(table3_df))\n",
        "\n",
        "# export to csv\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "date=dates.strftime('%Y-%m-%d')\n",
        "start=str(date[0])\n",
        "end=str(date[-1])\n",
        "name='{}_{}_{}-{}{}'.format('PerDay',customer,start,end,'.csv')\n",
        "table3_df.to_csv(name, index = False) \n",
        "files.download(name)\n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            ts  campaign_id  ...  group size  ad spend\n",
            "0   2019-08-20        18784  ...         437     0.000\n",
            "1   2019-08-20        18784  ...        3727    43.800\n",
            "2   2019-08-20        18789  ...        1321     0.000\n",
            "3   2019-08-20        18789  ...       12271    24.700\n",
            "4   2019-08-21        18784  ...         449     0.000\n",
            "5   2019-08-21        18784  ...        3768    41.800\n",
            "6   2019-08-21        18789  ...        1278     0.000\n",
            "7   2019-08-21        18789  ...       11955    26.360\n",
            "8   2019-08-22        18784  ...         460     0.000\n",
            "9   2019-08-22        18784  ...        3754    44.200\n",
            "10  2019-08-22        18789  ...        1283     0.000\n",
            "11  2019-08-22        18789  ...       11804    23.260\n",
            "12  2019-08-23        18784  ...         457     0.000\n",
            "13  2019-08-23        18784  ...        3747    41.800\n",
            "14  2019-08-23        18789  ...        1348     0.000\n",
            "15  2019-08-23        18789  ...       12542    26.620\n",
            "\n",
            "[16 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQGKxdgpr_W7",
        "colab_type": "text"
      },
      "source": [
        "## Export to overview sheet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocoZK_-ysBa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def overview_row(customer, audiences, dates, total):\n",
        "    return list([\n",
        "        customer,\n",
        "        \",\".join(audiences),\n",
        "        dates[0].strftime('%Y-%m-%d'),\n",
        "        dates[-1].strftime('%Y-%m-%d'),\n",
        "        __version__,\n",
        "    ]) + list(total.values)\n",
        "\n",
        "def export_to_overview(customer, audiences, dates, report):\n",
        "    !pip install --upgrade --quiet gspread\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    import gspread\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "    worksheet = gc.open('Uplift Report Overview').sheet1\n",
        "    row = overview_row(customer, audiences, dates, report['total'])\n",
        "    worksheet.append_row(row)\n",
        "   \n",
        "export_to_overview(customer, audiences, dates, report)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}